{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep probabilistic models with applications in astronomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import YouTubeVideo\n",
    "from tqdm.notebook import tqdm\n",
    "import corner\n",
    "\n",
    "import torch\n",
    "import pyro\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"Pyro version: {pyro.__version__}\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from astro_utils import plot_params, plot_lc, plot_lc_folded, featurize_lc, plot_lc_features, make_train_plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyro basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Bayesian linear regression \n",
    "\n",
    "$$\n",
    "y_i = w x_i + b +  \\epsilon_i\n",
    "$$\n",
    "\n",
    "with $N$ observations $(x_i, y_i)$, Gaussian noise and Gaussian priors for $w$ and  $b$\n",
    "\n",
    "The generative process in this case is \n",
    "\n",
    "- Sample $w \\sim \\mathcal{N}(\\mu_w, \\sigma_w^2)$\n",
    "- Sample $b \\sim \\mathcal{N}(\\mu_b, \\sigma_b^2)$\n",
    "- For $i=1,2,\\ldots, N$\n",
    "    - Sample $y_i \\sim \\mathcal{N}(w x_i + b, \\sigma_\\epsilon^2)$\n",
    "    \n",
    "where $\\mu_w, \\sigma_w, \\mu_b, \\sigma_b, \\sigma_\\epsilon$ are hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Writing a model\n",
    "\n",
    "To write the model we use the submodules and primitives\n",
    "\n",
    "- `pyro.distributions` to define prior/likelihoods \n",
    "- `pyro.sample` to define random variables (RV): Expects name, distribution and optionally observations\n",
    "- `pyro.plate` for conditionally independent RV: Expects name, and size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.distributions import Normal, Uniform\n",
    "\n",
    "def model(x, y=None): \n",
    "    w = pyro.sample(\"w\", Normal(0.0, 1.0))\n",
    "    b = pyro.sample(\"b\", Normal(0.0, 1.0))\n",
    "    s = pyro.sample(\"s\", Uniform(0.0, 1.0))    \n",
    "    with pyro.plate('dataset', size=len(x)):\n",
    "        return pyro.sample(\"y\", Normal(x*w + b, s), obs=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use \n",
    "\n",
    "- `pyro.infer.Predictive` \n",
    "\n",
    "to draw samples from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive = pyro.infer.Predictive(model, num_samples=500)\n",
    "\n",
    "hatx = np.linspace(-6, 6, num=100).astype('float32') \n",
    "apriori_samples = predictive(torch.from_numpy(hatx))\n",
    "\n",
    "# Plot samples from the priors\n",
    "figure = corner.corner(np.stack([apriori_samples[var].detach().numpy()[:, 0] for var in ['b', 'w', 's']]).T, \n",
    "                       smooth=1., labels=[\"bias\", \"weight\", \"noise_std\"], bins=20, \n",
    "                       quantiles=[0.16, 0.5, 0.84], \n",
    "                       show_titles=True, title_kwargs={\"fontsize\": 12})\n",
    "\n",
    "# Plot posterior predictive of y given x\n",
    "y_trace = apriori_samples[\"y\"].detach().numpy()\n",
    "med = np.median(y_trace, axis=0)\n",
    "qua = np.quantile(y_trace, (0.05, 0.95), axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "ax.plot(hatx, y_trace.T);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "In the bayesian setting we want the posterior distribution \n",
    "\n",
    "$$\n",
    "p(\\theta | \\mathcal{D}) = \\frac{p(\\mathcal{D}|\\theta) p(\\theta)}{\\int_\\theta p(\\mathcal{D}|\\theta) p(\\theta)}\n",
    "$$\n",
    "\n",
    "where $\\mathcal{D}$ is our dataset and $\\theta = (w, b)$\n",
    "\n",
    "For complex models the posterior is intractable. So we either do\n",
    "\n",
    "- MCMC: Train a Markov chain to generate samples as if they came from the actual posterior: Sampling based\n",
    "- Variational Inference: Choose a more simple posterior that is similar to the actual posterior: Optimization based\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Inference\n",
    "\n",
    "Propose an approximate (simple) posterior $q_\\phi(\\theta)$, e.g. factorized Gaussian\n",
    "\n",
    "Optimize $\\phi$ so that $q_\\phi$ approximates $p(\\theta|\\mathcal{D})$\n",
    "\n",
    "This is typically done by maximizing a lower bound on the evidence\n",
    "\n",
    "$$\n",
    "\\mathcal{ELBO}(\\phi) = \\mathbb{E}_{\\theta \\sim q_\\phi}[ \\log p(\\mathcal{D}|\\theta)] - \\text{KL}[q_\\phi(\\theta)|p(\\theta)]\n",
    "$$\n",
    "\n",
    "- Maximize the likelihood of the model\n",
    "- Minimize the distance between the approximate posterior and the prior\n",
    "\n",
    "Once $q$ has been trained we use it as a replacement for $p(\\theta|\\mathcal{D})$ to calculate the **posterior predictive distribution**\n",
    "\n",
    "$$\n",
    "p(\\mathbf{y}|\\mathbf{x}, \\mathcal{D}) = \\int p(\\mathbf{y}|\\mathbf{x}, \\theta) p(\\theta| \\mathcal{D}) \\,d\\theta\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI with Pyro\n",
    "\n",
    "We use `pyro.infer.SVI` to perform **Stochastic Variational Inference**, which expects\n",
    "\n",
    "- A generative model\n",
    "- An approximate posterior (guide)\n",
    "- Cost function: Typically ELBO\n",
    "- Optimizer: How to optimize the ELBO, typically gradient descent based\n",
    "\n",
    "We can use the `pyro.infer.autoguide` to create approximate posteriors from predefined recipes, for example a factorized Gaussian posterior (`AutoDiagonalNormal`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.enable_validation(True) # Useful for debugging\n",
    "pyro.clear_param_store()\n",
    "\n",
    "# Create a guide (approximate posterior)\n",
    "from pyro.infer.autoguide import AutoDiagonalNormal as approx_posterior\n",
    "guide = approx_posterior(model)\n",
    "\n",
    "\n",
    "# Stochastic Variational Inference\n",
    "svi = pyro.infer.SVI(model=model,  \n",
    "                     guide=guide,\n",
    "                     loss=pyro.infer.Trace_ELBO(), \n",
    "                     optim=pyro.optim.ClippedAdam({\"lr\": 1e-2}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets consider a dataset with two observations $\\mathcal{D} = \\{ (-2, -2), (2, 2) \\}$\n",
    "\n",
    "`svi.step(x, y)` performs a gradient ascent step to maximize the ELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(10, 2.5), dpi=80, tight_layout=True)\n",
    "\n",
    "nepochs = 1000\n",
    "loss = np.zeros(shape=(nepochs, ))\n",
    "params = np.zeros(shape=(nepochs, 2, 3))\n",
    "\n",
    "# Observed data\n",
    "x = torch.tensor([-2., 2.])\n",
    "y = torch.tensor([-2., 2.])\n",
    "\n",
    "for k in tqdm(range(nepochs)):\n",
    "    loss[k] = svi.step(x, y)\n",
    "    \n",
    "    phi = [param.detach().numpy() for param in guide.parameters()]\n",
    "    params[k, 0, :] = phi[0] # Locations\n",
    "    params[k, 1, :] = phi[1] # Scales    \n",
    "    if np.mod(k, 10) == 0:\n",
    "        plot_params(ax, k+1, loss, params)\n",
    "        fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we can use `pyro.infer.Predictive` to draw samples from the model\n",
    "\n",
    "This time we use the guide to sample $w$ and $b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive = pyro.infer.Predictive(model, \n",
    "                                   guide=guide, \n",
    "                                   num_samples=1000)\n",
    "\n",
    "posterior_samples = predictive(torch.from_numpy(hatx))\n",
    "\n",
    "# Plot posterior of w,  b and s\n",
    "figure = corner.corner(np.stack([posterior_samples[var].detach().numpy()[:, 0] for var in ['b', 'w', 's']]).T, \n",
    "                       smooth=1., labels=[\"bias\", \"weight\", \"noise_std\"], bins=20, \n",
    "                       quantiles=[0.16, 0.5, 0.84],\n",
    "                       show_titles=True, title_kwargs={\"fontsize\": 12})\n",
    "\n",
    "# Plot posterior predictive of y given x\n",
    "y_trace = posterior_samples[\"y\"].detach().numpy()\n",
    "med = np.median(y_trace, axis=0)\n",
    "qua = np.quantile(y_trace, (0.05, 0.95), axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "ax.plot(hatx, med)\n",
    "ax.fill_between(hatx, qua[0], qua[1], alpha=0.5);\n",
    "\n",
    "ax.errorbar(x.numpy(), y.numpy(), yerr=2*posterior_samples['s'].median().item(), \n",
    "           fmt='none', c='k', zorder=100);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artificial Neuron: Linear regressor plus activation function (e.g. sigmoid -> Logistic regression)\n",
    "\n",
    "$$\n",
    "\\hat y = g\\left(\\sum_i w_i x_i + b\\right)\n",
    "$$\n",
    "\n",
    "Multilayer perceptron: Several fully connected layers concatenated\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f_i &=   b_i + \\sum_{j=1}^H w_{ij} h_j  \\nonumber \\\\\n",
    "&=  b_i + \\sum_{j=1}^H w_{ij} g \\left( b_j + \\sum_{d=1}^D w_{jd} x_d  \\right) \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "<img src=\"images/MLP.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network model in pytorch\n",
    "\n",
    "- We write a class that inherits from `torch.nn.Module`\n",
    "- The constructor `__init__(self, args):` define the layers, e.g. fully-connected (`Linear`), convolutional (`Conv1D`, `Conv2D`)\n",
    "- The function `forward(self, x):` defines how layers are connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, D, H, O):\n",
    "        super(type(self), self).__init__()\n",
    "        self.hidden_layer = torch.nn.Linear(D, H) # WX + B\n",
    "        self.output_layer = torch.nn.Linear(H, O)\n",
    "        self.activation = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x): \n",
    "        z = self.activation(self.hidden_layer(x))\n",
    "        return self.output_layer(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder (VAE) [(Kingma et al., 2014)](https://arxiv.org/abs/1312.6114)\n",
    "\n",
    "Non-linear latent variable model\n",
    "\n",
    "<img src=\"images/VAE.png\" width=\"800\">\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Latent variables: $z \\sim \\mathcal{N}(0, I)$\n",
    "- Observed variable: $x|z \\sim \\mathcal{N}(\\hat \\mu, \\hat \\sigma^2)$\n",
    "\n",
    "We use a neural network $f_\\theta$ to model a non-linear function $\\hat \\mu (z)$\n",
    "\n",
    "We want to infer the latent given the observed\n",
    "\n",
    "$$\n",
    "p(z|x) = \\frac{p(x|z) p(z)}{\\int p(x|z) p(z) dz}\n",
    "$$\n",
    "\n",
    "But this is not tractable so we use a variational approximation (factorized Gaussian)\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "q_\\phi(z|x) &= \\mathcal{N}(\\mu(x), \\sigma(x)^2) \\nonumber \\\\\n",
    "&= \\mu(x) + \\sigma(x) \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I) \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where a neural network $g_\\phi$ are used to model the non-linear functions $\\mu(x)$ and $\\sigma(x)$\n",
    "\n",
    "We amortize the parameters of $q$ using the neural network, making inference very efficient\n",
    "\n",
    "Note: The VAE is not a full bayesian neural network, the parameters of the neural net are point estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.distributions import Normal\n",
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, data_dim, latent_dim, hidden_dim):\n",
    "        super(type(self), self).__init__()\n",
    "        self.hidden = nn.Linear(data_dim, hidden_dim)\n",
    "        self.z_loc = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.z_scale = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.activation = nn.Softplus()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.activation(self.hidden(x))\n",
    "        return self.z_loc(h), self.activation(self.z_scale(h))\n",
    "    \n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, data_dim, latent_dim, hidden_dim):\n",
    "        super(type(self), self).__init__()\n",
    "        self.hidden = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.x_loc = nn.Linear(hidden_dim, data_dim)\n",
    "        self.activation = nn.Softplus()\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = self.activation(self.hidden(z))\n",
    "        return self.x_loc(h)\n",
    "    \n",
    "class VariationalAutoEncoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, data_dim, latent_dim, hidden_dim, sigmax):\n",
    "        super(VariationalAutoEncoder, self).__init__() \n",
    "        self.encoder = Encoder(data_dim, latent_dim, hidden_dim)\n",
    "        self.decoder = Decoder(data_dim, latent_dim, hidden_dim)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.data_dim = data_dim\n",
    "        self.sigmax = sigmax\n",
    "        \n",
    "    def model(self, x):\n",
    "        pyro.module(\"decoder\", self.decoder) \n",
    "        with pyro.plate(\"data\", size=x.shape[0]):\n",
    "            # p(z)\n",
    "            z_loc = torch.zeros(x.shape[0], self.latent_dim, device=x.device)\n",
    "            z_scale = torch.ones(x.shape[0], self.latent_dim, device=x.device)\n",
    "            z = pyro.sample(\"latent\", Normal(z_loc, z_scale).to_event(1))\n",
    "            # p(x|z)\n",
    "            x_loc = self.decoder.forward(z)\n",
    "            pyro.sample(\"observed\", Normal(x_loc, self.sigmax).to_event(1), obs=x)\n",
    "            return x_loc\n",
    "    \n",
    "    def guide(self, x):\n",
    "        pyro.module(\"encoder\", self.encoder)\n",
    "        with pyro.plate(\"data\", size=x.shape[0]):\n",
    "            # q(z|x)\n",
    "            z_loc, z_scale  = self.encoder.forward(x)\n",
    "            pyro.sample(\"latent\", Normal(z_loc, z_scale).to_event(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Astronomical time series\n",
    "\n",
    "\n",
    "\n",
    "- **Light curve:** Time series of a star's flux (brightness)\n",
    "- We will consider two optical passbands\n",
    "- The \"apparent\" brightness is estimated through **Photometry**\n",
    "- Main tool for variable star studies\n",
    "\n",
    "<table><tr><td>\n",
    "    <img src=\"images/ZTF.png\" width=\"250\">\n",
    "</td><td>\n",
    "    <img src=\"images/intro-sources.png\" width=\"300\">\n",
    "</td></tr></table>\n",
    "\n",
    "<center>\n",
    "    <img src=\"images/intro-sources-time.png\" width=\"600\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import pickle\n",
    "\n",
    "with bz2.BZ2File(\"lcdata.pbz2\", 'r') as f:\n",
    "    lcs, periods, labels = pickle.load(f)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "plot_lc(ax, lcs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Light curve \"features\": Irregular sampling, gaps in observations, [heteroscedastic](https://en.wikipedia.org/wiki/Heteroscedasticity) noise\n",
    "- **Variable stars**: Brightness change in time either regularly or stochastically\n",
    "- Some variable stars are radial pulsators. They expand/heat and contract/cool regularly. Examples: RR Lyrae and Cepheid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo('sXJBrRmHPj8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we know the pulsation period we can use the **epoch folding transformation** to obtain a phase diagram\n",
    "\n",
    "<img src=\"images/folding.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fold(time, period):\n",
    "    \"\"\"\n",
    "    returns phase = time/period - floor(time/period)\n",
    "    \"\"\"\n",
    "    return np.mod(time, period)/period\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "plot_lc_folded(ax, lcs[0], periods[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a VAE for periodic light curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very simple preprocessing and data preparation routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pha_interp = np.linspace(0, 1, num=40)\n",
    "mag_interp = np.zeros(shape=(len(lcs), 2, len(pha_interp)))\n",
    "err_interp = np.zeros(shape=(len(lcs), 2, len(pha_interp)))\n",
    "\n",
    "for k, (lc, period) in enumerate(zip(lcs, periods)):\n",
    "    mag_interp[k], err_interp[k], stats = featurize_lc(lc, period, pha_interp)\n",
    "    \n",
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "plot_lc_features(ax, pha_interp, mag_interp[0], err_interp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder  \n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "\n",
    "# reproducibility\n",
    "torch.manual_seed(12345);  \n",
    "np.random.seed(12345) \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 2), tight_layout=True)\n",
    "ax.hist(labels, bins=np.arange(-0.5, 5.5, step=1), rwidth=0.8);\n",
    "\n",
    "le = LabelEncoder()\n",
    "labels_int = le.fit_transform(labels)\n",
    "# Create light curve dataset from numpy arrays\n",
    "lc_dataset = TensorDataset(torch.from_numpy(mag_interp.astype('float32')), \n",
    "                           torch.from_numpy(err_interp.astype('float32'))\n",
    "                           )\n",
    "\n",
    "# Generate data loaders\n",
    "idx = np.random.permutation(len(lcs))\n",
    "train_loader = DataLoader(dataset=Subset(lc_dataset, idx[len(lcs)//5:]), \n",
    "                          batch_size=32, shuffle=True)\n",
    "                          \n",
    "valid_loader = DataLoader(dataset=Subset(lc_dataset, idx[:len(lcs)//5]), batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a VAE that receives two inputs (r and g) and generates two outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.distributions import Normal\n",
    "import torch.nn as nn\n",
    "\n",
    "class VariationalAutoEncoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, data_dim, latent_dim, hidden_dim):\n",
    "        super(VariationalAutoEncoder, self).__init__() \n",
    "        self.encoder = Encoder(data_dim, latent_dim, hidden_dim)\n",
    "        self.decoder = Decoder(data_dim, latent_dim, hidden_dim)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.data_dim = data_dim\n",
    "        \n",
    "    def model(self, mags, errs):\n",
    "        pyro.module(\"decoder\", self.decoder)\n",
    "        with pyro.plate(\"minibatch\", size=mags.shape[0]):\n",
    "            # p(z)\n",
    "            z_loc = torch.zeros(mags.shape[0], self.latent_dim, device=mags.device)\n",
    "            z_scale = torch.ones(mags.shape[0], self.latent_dim, device=mags.device)\n",
    "            z = pyro.sample(\"latent\", Normal(z_loc, z_scale).to_event(1))\n",
    "            # p(x|z)\n",
    "            x_loc1, x_loc2 = self.decoder.forward(z)\n",
    "            pyro.sample(\"observed_g\", Normal(x_loc1, errs[:, 0, :]).to_event(1), obs=mags[:, 0, :])\n",
    "            pyro.sample(\"observed_r\", Normal(x_loc2, errs[:, 1, :]).to_event(1), obs=mags[:, 1, :])\n",
    "            return x_loc1, x_loc2\n",
    "    \n",
    "    def guide(self, mags, errs):\n",
    "        pyro.module(\"encoder\", self.encoder)\n",
    "        with pyro.plate(\"minibatch\", size=mags.shape[0]):\n",
    "            # q(z|x)\n",
    "            z_loc, z_scale  = self.encoder.forward(mags)\n",
    "            pyro.sample(\"latent\", Normal(z_loc, z_scale).to_event(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, data_dim, latent_dim, hidden_dim):\n",
    "        super(type(self), self).__init__()\n",
    "        self.hidden1_g = nn.Linear(data_dim, hidden_dim)\n",
    "        self.hidden1_r = nn.Linear(data_dim, hidden_dim)\n",
    "        self.hidden2 = nn.Linear(hidden_dim*2, hidden_dim)\n",
    "        self.z_loc = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.z_scale = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.activation = nn.Softplus()\n",
    "\n",
    "    def forward(self, x):\n",
    "        hg = self.activation(self.hidden1_g(x[:, 0, :]))\n",
    "        hr = self.activation(self.hidden1_r(x[:, 1, :]))\n",
    "        h = torch.cat([hg, hr], dim=1)\n",
    "        h = self.activation(self.hidden2(h))        \n",
    "        return self.z_loc(h), self.activation(self.z_scale(h))\n",
    "    \n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, data_dim, latent_dim, hidden_dim):\n",
    "        super(type(self), self).__init__()\n",
    "        self.hidden1 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.hidden2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.x_locg = nn.Linear(hidden_dim, data_dim)\n",
    "        self.x_locr = nn.Linear(hidden_dim, data_dim)\n",
    "        self.activation = nn.Softplus()\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = self.activation(self.hidden1(z))\n",
    "        h = self.activation(self.hidden2(h))\n",
    "        return self.x_locg(h), self.x_locr(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model and SVI object definitions and training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.enable_validation(True) \n",
    "pyro.clear_param_store()\n",
    "\n",
    "vae = VariationalAutoEncoder(data_dim=40, latent_dim=2, hidden_dim=20).cuda()\n",
    "    \n",
    "svi = pyro.infer.SVI(model=vae.model, \n",
    "                     guide=vae.guide, \n",
    "                     optim=pyro.optim.Adam({\"lr\": 1e-3}), \n",
    "                     loss=pyro.infer.Trace_ELBO())\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 3), tight_layout=True)\n",
    "\n",
    "nepochs = 100\n",
    "epoch_loss = np.zeros(shape=(nepochs, 2))\n",
    "for nepoch in tqdm(range(nepochs)):\n",
    "    \n",
    "    # Training loop\n",
    "    for mags, errs in train_loader:\n",
    "        epoch_loss[nepoch, 0] += svi.step(mags.cuda(), errs.cuda())\n",
    "    epoch_loss[nepoch, 0] /= len(train_loader.dataset)\n",
    "    \n",
    "    # Validation loop\n",
    "    for mags, errs in valid_loader:\n",
    "        epoch_loss[nepoch, 1] += svi.evaluate_loss(mags.cuda(), errs.cuda())\n",
    "    epoch_loss[nepoch, 1] /= len(valid_loader.dataset)    \n",
    "        \n",
    "    # Plot latent space and learning curves\n",
    "    Z = torch.cat((vae.encoder(lc_dataset.tensors[0].cuda())), dim=1)\n",
    "    Z = Z.detach().cpu().numpy()\n",
    "    make_train_plots(ax, nepoch, Z, labels_int, epoch_loss)\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input and reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = vae.cpu()\n",
    "    \n",
    "fig, ax = plt.subplots(2, 10, figsize=(8, 2), tight_layout=True)\n",
    "\n",
    "mags, errs = next(iter(valid_loader))\n",
    "z_loc, z_scale = vae.encoder.forward(mags)\n",
    "mug, mur = vae.decoder(z_loc)\n",
    "for k, (c, mu) in enumerate(zip(['g', 'r'], [mug, mur])):\n",
    "    for i in range(10):        \n",
    "        ax[k, i].plot(mags.numpy()[i, k, :], c=c)\n",
    "        ax[k, i].plot(mu[i].detach().numpy(), ls='--', c=c)\n",
    "        ax[k, i].invert_yaxis(); \n",
    "        ax[k, i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent space visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6), tight_layout=True)\n",
    "\n",
    "for k, name in zip(np.unique(labels_int), np.unique(labels)):\n",
    "    mask = labels_int == k\n",
    "    ax.errorbar(x=Z[mask, 0], y=Z[mask, 1], \n",
    "                xerr=Z[mask, 2], yerr=Z[mask, 3],\n",
    "                fmt='none', alpha=0.5, label=name)\n",
    "ax.legend(loc=4)\n",
    "ax.set_xlim([-5., 5.])\n",
    "ax.set_ylim([-5., 5.]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent space interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 10\n",
    "z_plot = np.linspace(-5., 5., num=M)\n",
    "\n",
    "fig, ax = plt.subplots(M, M, figsize=(8, 6), tight_layout=True)\n",
    "for i in range(M):\n",
    "    for j in range(M):\n",
    "        z = torch.tensor(np.array([z_plot[j], z_plot[M-1-i]]), dtype=torch.float32)\n",
    "        mug, mur = vae.decoder.forward(z)\n",
    "        mug = mug.detach().numpy()\n",
    "        mur = mur.detach().numpy()\n",
    "        ax[i, j].plot(mug, lw=2, c='g')\n",
    "        ax[i, j].plot(mur, lw=2, c='r')\n",
    "        ax[i, j].invert_yaxis(); \n",
    "        ax[i, j].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interested in Astroinformatics?\n",
    "\n",
    "<a href=\"http://alerce.science\"><img src=\"images/alerce.png\" width=\"600\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
